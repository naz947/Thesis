{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import json, re, nltk, string\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_bugs_json = '/home/data/chrome/deep_data.json'\n",
    "closed_bugs_json = '/home/data/chrome/classifier_data_0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Word2vec parameters\n",
    "min_word_frequency_word2vec = 5\n",
    "embed_size_word2vec = 200\n",
    "context_window_word2vec = 5\n",
    "\n",
    "#2. Classifier hyperparameters\n",
    "numCV = 10\n",
    "max_sentence_len = 50\n",
    "min_sentence_length = 15\n",
    "rankK = 10\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(open_bugs_json) as data_file:\n",
    "    data = json.load(data_file, strict=False)\n",
    "\n",
    "all_data = []\n",
    "for item in data:\n",
    "    #1. Remove \\r \n",
    "    current_title = item['issue_title'].replace('\\r', ' ')\n",
    "    current_desc = item['description'].replace('\\r', ' ')    \n",
    "    #2. Remove URLs\n",
    "    current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)    \n",
    "    #3. Remove Stack Trace\n",
    "    start_loc = current_desc.find(\"Stack trace:\")\n",
    "    current_desc = current_desc[:start_loc]    \n",
    "    #4. Remove hex code\n",
    "    current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc)\n",
    "    current_title= re.sub(r'(\\w+)0x\\w+', '', current_title)    \n",
    "    #5. Change to lower case\n",
    "    current_desc = current_desc.lower()\n",
    "    current_title = current_title.lower()    \n",
    "    #6. Tokenize\n",
    "    current_desc_tokens = nltk.word_tokenize(current_desc)\n",
    "    current_title_tokens = nltk.word_tokenize(current_title)\n",
    "    #7. Strip trailing punctuation marks    \n",
    "    current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]\n",
    "    current_title_filter = [word.strip(string.punctuation) for word in current_title_tokens]      \n",
    "    #8. Join the lists\n",
    "    current_data = current_title_filter + current_desc_filter\n",
    "    current_data = filter(None, current_data)\n",
    "    all_data.append(current_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_model = Word2Vec(all_data, min_count=min_word_frequency_word2vec, size=embed_size_word2vec, window=context_window_word2vec)\n",
    "vocabulary = wordvec_model.vocab\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(closed_bugs_json) as data_file:\n",
    "    data = json.load(data_file, strict=False)\n",
    "\n",
    "all_data = []\n",
    "all_owner = []    \n",
    "for item in data:\n",
    "    #1. Remove \\r \n",
    "    current_title = item['issue_title'].replace('\\r', ' ')\n",
    "    current_desc = item['description'].replace('\\r', ' ')\n",
    "    #2. Remove URLs\n",
    "    current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)\n",
    "    #3. Remove Stack Trace\n",
    "    start_loc = current_desc.find(\"Stack trace:\")\n",
    "    current_desc = current_desc[:start_loc]\n",
    "    #4. Remove hex code\n",
    "    current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc)\n",
    "    current_title= re.sub(r'(\\w+)0x\\w+', '', current_title)\n",
    "    #5. Change to lower case\n",
    "    current_desc = current_desc.lower()\n",
    "    current_title = current_title.lower()\n",
    "    #6. Tokenize\n",
    "    current_desc_tokens = nltk.word_tokenize(current_desc)\n",
    "    current_title_tokens = nltk.word_tokenize(current_title)\n",
    "    #7. Strip punctuation marks\n",
    "    current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]\n",
    "    current_title_filter = [word.strip(string.punctuation) for word in current_title_tokens]       \n",
    "    #8. Join the lists\n",
    "    current_data = current_title_filter + current_desc_filter\n",
    "    current_data = filter(None, current_data)\n",
    "    all_data.append(current_data)\n",
    "    all_owner.append(item['owner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalLength = len(all_data)\n",
    "splitLength = totalLength / (numCV + 1)\n",
    "\n",
    "for i in range(1, numCV+1):\n",
    "    train_data = all_data[:i*splitLength-1]\n",
    "    test_data = all_data[i*splitLength:(i+1)*splitLength-1]\n",
    "    train_owner = all_owner[:i*splitLength-1]\n",
    "    test_owner = all_owner[i*splitLength:(i+1)*splitLength-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1 # Denotes the cross validation set number\n",
    "updated_train_data = []    \n",
    "updated_train_data_length = []    \n",
    "updated_train_owner = []\n",
    "final_test_data = []\n",
    "final_test_owner = []\n",
    "for j, item in enumerate(train_data):\n",
    "    current_train_filter = [word for word in item if word in vocabulary]\n",
    "    if len(current_train_filter)>=min_sentence_length:  \n",
    "        updated_train_data.append(current_train_filter)\n",
    "        updated_train_owner.append(train_owner[j])  \n",
    "        \n",
    "        \n",
    "for j, item in enumerate(test_data):\n",
    "    current_test_filter = [word for word in item if word in vocabulary]  \n",
    "    if len(current_test_filter)>=min_sentence_length:\n",
    "        final_test_data.append(current_test_filter)    \t  \n",
    "        final_test_owner.append(test_owner[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1 # Denotes the cross validation set number\n",
    "# Remove data from test set that is not there in train set\n",
    "train_owner_unique = set(updated_train_owner)\n",
    "test_owner_unique = set(final_test_owner)\n",
    "unwanted_owner = list(test_owner_unique - train_owner_unique)\n",
    "updated_test_data = []\n",
    "updated_test_owner = []\n",
    "updated_test_data_length = []\n",
    "for j in range(len(final_test_owner)):\n",
    "    if final_test_owner[j] not in unwanted_owner:\n",
    "        updated_test_data.append(final_test_data[j])\n",
    "        updated_test_owner.append(final_test_owner[j])\n",
    "\n",
    "unique_train_label = list(set(updated_train_owner))\n",
    "classes = np.array(unique_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.empty(shape=[len(updated_train_data), max_sentence_len, embed_size_word2vec], dtype='float32')\n",
    "Y_train = np.empty(shape=[len(updated_train_owner),1], dtype='int32')\n",
    "# 1 - start of sentence, # 2 - end of sentence, # 0 - zero padding. Hence, word indices start with 3 \n",
    "for j, curr_row in enumerate(updated_train_data):\n",
    "\tsequence_cnt = 0         \n",
    "\tfor item in curr_row:\n",
    "\t\tif item in vocabulary:\n",
    "\t\t\tX_train[j, sequence_cnt, :] = wordvec_model[item] \n",
    "\t\t\tsequence_cnt = sequence_cnt + 1                \n",
    "\t\t\tif sequence_cnt == max_sentence_len-1:\n",
    "\t\t\t\t\tbreak                \n",
    "\tfor k in range(sequence_cnt, max_sentence_len):\n",
    "\t\tX_train[j, k, :] = np.zeros((1,embed_size_word2vec))        \n",
    "\tY_train[j,0] = unique_train_label.index(updated_train_owner[j])\n",
    "\n",
    "X_test = np.empty(shape=[len(updated_test_data), max_sentence_len, embed_size_word2vec], dtype='float32')\n",
    "Y_test = np.empty(shape=[len(updated_test_owner),1], dtype='int32')\n",
    "# 1 - start of sentence, # 2 - end of sentence, # 0 - zero padding. Hence, word indices start with 3 \n",
    "for j, curr_row in enumerate(updated_test_data):\n",
    "\tsequence_cnt = 0          \n",
    "\tfor item in curr_row:\n",
    "\t\tif item in vocabulary:\n",
    "\t\t\tX_test[j, sequence_cnt, :] = wordvec_model[item] \n",
    "\t\t\tsequence_cnt = sequence_cnt + 1                \n",
    "\t\t\tif sequence_cnt == max_sentence_len-1:\n",
    "\t\t\t\t\tbreak                \n",
    "\tfor k in range(sequence_cnt, max_sentence_len):\n",
    "\t\tX_test[j, k, :] = np.zeros((1,embed_size_word2vec))        \n",
    "\tY_test[j,0] = unique_train_label.index(updated_test_owner[j])\n",
    "\t\n",
    "y_train = np_utils.to_categorical(Y_train, len(unique_train_label))\n",
    "y_test = np_utils.to_categorical(Y_test, len(unique_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_sentence_len,), dtype='int32')\n",
    "sequence_embed = Embedding(vocab_size, embed_size_word2vec, input_length=max_sentence_len)(input)\n",
    "\n",
    "forwards_1 = LSTM(1024, return_sequences=True, dropout_U=0.2)(sequence_embed)\n",
    "attention_1 = SoftAttentionConcat()(forwards_1)\n",
    "after_dp_forward_5 = BatchNormalization()(attention_1)\n",
    "\n",
    "backwards_1 = LSTM(1024, return_sequences=True, dropout_U=0.2, go_backwards=True)(sequence_embed)\n",
    "attention_2 = SoftAttentionConcat()(backwards_1)\n",
    "after_dp_backward_5 = BatchNormalization()(attention_2)\n",
    "\t\t\t \n",
    "merged = merge([after_dp_forward_5, after_dp_backward_5], mode='concat', concat_axis=-1)\n",
    "after_merge = Dense(1000, activation='relu')(merged)\n",
    "after_dp = Dropout(0.4)(after_merge)\n",
    "output = Dense(len(train_label), activation='softmax')(after_dp)                \n",
    "model = Model(input=input, output=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_safe(x):\n",
    "    return K.clip(x, K.common._EPSILON, 1.0 - K.common._EPSILON)\n",
    "\n",
    "class ProbabilityTensor(Wrapper):\n",
    "    \"\"\" function for turning 3d tensor to 2d probability matrix, which is the set of a_i's \"\"\"\n",
    "    def __init__(self, dense_function=None, *args, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        #layer = TimeDistributed(dense_function) or TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        layer = TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        super(ProbabilityTensor, self).__init__(layer, *args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        if K._BACKEND == 'tensorflow':\n",
    "            if not input_shape[1]:\n",
    "                raise Exception('When using TensorFlow, you should define '\n",
    "                                'explicitly the number of timesteps of '\n",
    "                                'your sequences.\\n'\n",
    "                                'If your first layer is an Embedding, '\n",
    "                                'make sure to pass it an \"input_length\" '\n",
    "                                'argument. Otherwise, make sure '\n",
    "                                'the first layer has '\n",
    "                                'an \"input_shape\" or \"batch_input_shape\" '\n",
    "                                'argument, including the time axis.')\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(ProbabilityTensor, self).build()\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        # b,n,f -> b,n \n",
    "        #       s.t. \\sum_n n = 1\n",
    "        if isinstance(input_shape, (list,tuple)) and not isinstance(input_shape[0], int):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        return (input_shape[0], input_shape[1])\n",
    "\n",
    "    def squash_mask(self, mask):\n",
    "        if K.ndim(mask) == 2:\n",
    "            return mask\n",
    "        elif K.ndim(mask) == 3:\n",
    "            return K.any(mask, axis=-1)\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        return self.squash_mask(mask)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        energy = K.squeeze(self.layer(x), 2)\n",
    "        p_matrix = K.softmax(energy)\n",
    "        if mask is not None:\n",
    "            mask = self.squash_mask(mask)\n",
    "            p_matrix = make_safe(p_matrix * mask)\n",
    "            p_matrix = (p_matrix / K.sum(p_matrix, axis=-1, keepdims=True))*mask\n",
    "        return p_matrix\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(ProbabilityTensor, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SoftAttentionConcat(ProbabilityTensor):\n",
    "    '''This will create the context vector and then concatenate it with the last output of the LSTM'''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        # b,n,f -> b,f where f is weighted features summed across n\n",
    "        return (input_shape[0], 2*input_shape[2])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None or mask.ndim==2:\n",
    "            return None\n",
    "        else:\n",
    "            raise Exception(\"Unexpected situation\")\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # b,n,f -> b,f via b,n broadcasted\n",
    "        p_vectors = K.expand_dims(super(SoftAttentionConcat, self).call(x, mask), 2)\n",
    "        expanded_p = K.repeat_elements(p_vectors, K.int_shape(x)[2], axis=2)\n",
    "        context = K.sum(expanded_p * x, axis=1)\n",
    "        last_out = x[:, -1, :]\n",
    "        return K.concatenate([context, last_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=200)              \n",
    "    \n",
    "predict = model.predict(X_test)        \n",
    "accuracy = []\n",
    "sortedIndices = []\n",
    "pred_classes = []\n",
    "for ll in predict:\n",
    "\tsortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n",
    "for k in range(1, rankK+1):\n",
    "\tid = 0\n",
    "\ttrueNum = 0\n",
    "\tfor sortedInd in sortedIndices:            \n",
    "\t\tpred_classes.append(classes[sortedInd[:k]])\n",
    "\t\tif y_test[id] in classes[sortedInd[:k]]:\n",
    "\t\t\t  trueNum += 1\n",
    "\t\tid += 1\n",
    "\taccuracy.append((float(trueNum) / len(predict)) * 100)\n",
    "print('Test accuracy:', accuracy)       \n",
    "\n",
    "train_result = hist.history        \n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for item in updated_train_data:\n",
    "\t  train_data.append(' '.join(item))\n",
    "\t \n",
    "test_data = []\n",
    "for item in updated_test_data:\n",
    "\t  test_data.append(' '.join(item))\n",
    "\n",
    "vocab_data = []\n",
    "for item in vocabulary:\n",
    "\t  vocab_data.append(item)\n",
    "\n",
    "# Extract tf based bag of words representation\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "count_vect = CountVectorizer(min_df=1, vocabulary= vocab_data,dtype=np.int32)\n",
    "\n",
    "train_counts = count_vect.fit_transform(train_data)       \n",
    "train_feats = tfidf_transformer.fit_transform(train_counts)\n",
    "print train_feats.shape\n",
    "\n",
    "test_counts = count_vect.transform(test_data)\n",
    "test_feats = tfidf_transformer.transform(test_counts)\n",
    "print test_feats.shape\n",
    "print \"=======================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierModel = MultinomialNB(alpha=0.01)        \n",
    "classifierModel = OneVsRestClassifier(classifierModel).fit(train_feats, updated_train_owner)\n",
    "predict = classifierModel.predict_proba(test_feats)  \n",
    "classes = classifierModel.classes_  \n",
    "\n",
    "accuracy = []\n",
    "sortedIndices = []\n",
    "pred_classes = []\n",
    "for ll in predict:\n",
    "\tsortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n",
    "for k in range(1, rankK+1):\n",
    "\tid = 0\n",
    "\ttrueNum = 0\n",
    "\tfor sortedInd in sortedIndices:            \n",
    "\t\tif y_test[id] in classes[sortedInd[:k]]:\n",
    "\t\t\ttrueNum += 1\n",
    "\t\t\tpred_classes.append(classes[sortedInd[:k]])\n",
    "\t\tid += 1\n",
    "\taccuracy.append((float(trueNum) / len(predict)) * 100)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierModel = svm.SVC(probability=True, verbose=False, decision_function_shape='ovr', random_state=42)\n",
    "classifierModel.fit(train_feats, updated_train_owner)\n",
    "predict = classifierModel.predict(test_feats)\n",
    "classes = classifierModel.classes_ \n",
    "\n",
    "accuracy = []\n",
    "sortedIndices = []\n",
    "pred_classes = []\n",
    "for ll in predict:\n",
    "\tsortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n",
    "for k in range(1, rankK+1):\n",
    "\tid = 0\n",
    "\ttrueNum = 0\n",
    "\tfor sortedInd in sortedIndices:            \n",
    "\t\tif y_test[id] in classes[sortedInd[:k]]:\n",
    "\t\t\ttrueNum += 1\n",
    "\t\t\tpred_classes.append(classes[sortedInd[:k]])\n",
    "\t\tid += 1\n",
    "\taccuracy.append((float(trueNum) / len(predict)) * 100)\n",
    "print accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = cosine_similarity(test_feats, train_feats)\n",
    "classes = np.array(trainls)\n",
    "classifierModel = []\n",
    "\n",
    "accuracy = []\n",
    "sortedIndices = []\n",
    "pred_classes = []\n",
    "for ll in predict:\n",
    "\tsortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n",
    "for k in range(1, rankK+1):\n",
    "\tid = 0\n",
    "\ttrueNum = 0\n",
    "\tfor sortedInd in sortedIndices:            \n",
    "\t\tif y_test[id] in classes[sortedInd[:k]]:\n",
    "\t\t\ttrueNum += 1\n",
    "\t\t\tpred_classes.append(classes[sortedInd[:k]])\n",
    "\t\tid += 1\n",
    "\taccuracy.append((float(trueNum) / len(predict)) * 100)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierModel = LogisticRegression(solver='lbfgs', penalty='l2', tol=0.01)\n",
    "classifierModel = OneVsRestClassifier(classifierModel).fit(train_feats, updated_train_owner)\n",
    "predict = classifierModel.predict(test_feats)\n",
    "classes = classifierModel.classes_ \n",
    "\n",
    "accuracy = []\n",
    "sortedIndices = []\n",
    "pred_classes = []\n",
    "for ll in predict:\n",
    "sortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n",
    "for k in range(1, rankK+1):\n",
    "id = 0\n",
    "trueNum = 0\n",
    "for sortedInd in sortedIndices:            \n",
    "\tif y_test[id] in classes[sortedInd[:k]]:\n",
    "\t\ttrueNum += 1\n",
    "\t\tpred_classes.append(classes[sortedInd[:k]])\n",
    "\tid += 1\n",
    "accuracy.append((float(trueNum) / len(predict)) * 100)\n",
    "print accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
